{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ProyectoSL2-P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenny08gt/proyecto_statistical_learning2/blob/master/ProyectoSL2_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FBLcUbkZwNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# device = 'cpu'\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/statistical learning 2/Proyecto/songdata.csv')\n",
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52pwGaaNZwNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, inputs_size)\n",
        "\n",
        "        self.rnn = nn.LSTM(inputs_size, num_hidden_nodes, num_layers, dropout=dropout)\n",
        "       \n",
        "        self.decoder = nn.Linear(num_hidden_nodes, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if num_hidden_nodes != inputs_size:\n",
        "                raise ValueError('When using the tied flag, num_hidden_nodes must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "        self.num_hidden_nodes = num_hidden_nodes\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.xavier_uniform_(self.encoder.weight)\n",
        "#         self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        nn.init.xavier_uniform_(self.decoder.weight)\n",
        "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden_nodes), weight.new_zeros(self.num_layers, batch_size, self.num_hidden_nodes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBgv70GJZwNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dictionary = Dictionary() \n",
        "        lyrics = dataframe['text'].apply(self.pre_process)\n",
        "        train, test, y_train, y_test = train_test_split(lyrics, dataframe['artist'], test_size=0.3, random_state=1)\n",
        "        train, val, y_train, y_val = train_test_split(train, y_train, test_size=0.3, random_state=1)\n",
        "        self.train = self.tokenize(train.str.cat(sep=' end_song '))\n",
        "        self.valid = self.tokenize(val.str.cat(sep=' end_song '))\n",
        "        self.test = self.tokenize(test.str.cat(sep=' end_song '))\n",
        "        self.train_raw = train.str.cat(sep=' end_song ')\n",
        "        self.valid_raw = val.str.cat(sep=' end_song ')\n",
        "        self.test_raw = test.str.cat(sep=' end_song ')\n",
        "        \n",
        "    def pre_process(self,text):\n",
        "        text = text.replace(\"\\r\",\" \")\n",
        "        text = text.replace(\"\\n\",\" \")\n",
        "        text = text.replace(\"x2\",\" \")\n",
        "        text = text.replace(\"x3\",\" \")\n",
        "        text = text.replace(\"'\",\" \")\n",
        "        text = text.replace(\"end_song\",\" \")\n",
        "        text = text.lower()\n",
        "        table = str.maketrans('', '', '!\"#$%&\\()*+-/:;<=>?@[\\\\]^_`{|}~')\n",
        "        text = text.translate(table)\n",
        "        return(text)\n",
        "    \n",
        "    def tokenize(self, string):\n",
        "        tokens = 0\n",
        "        words = nltk.word_tokenize(string)\n",
        "        tokens += len(words)\n",
        "        for word in words:\n",
        "            self.dictionary.add_word(word)\n",
        "\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        words = nltk.word_tokenize(string)\n",
        "        for word in words:\n",
        "            ids[token] = self.dictionary.word2idx[word]\n",
        "            token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpUzNHm6ZwNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createCorpus(subset):\n",
        "  return Corpus(subset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "desuAPBaZwNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "  \n",
        "    nbatch = data.size(0) // batch_size\n",
        "#     print(\"batch_size: \" + str(batch_size))\n",
        "#     print(\"batches: \" + str(nbatch))\n",
        "    \n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4xz8HhWZwN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "    \n",
        "def train(epoch, batch_size):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    cur_loss = 0\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        \n",
        "        # Reset the gradient after every epoch. \n",
        "#         optimizer.zero_grad()      \n",
        "        model.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimizer take a step and update the weights.\n",
        "#         optimizer.step()\n",
        "        \n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "#         print(str(args['log_interval']) + \", batch \" + str(batch))\n",
        "        if batch % args['log_interval'] == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args['log_interval']\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:2.8f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.8f}'.format(\n",
        "                epoch, batch, len(train_data) // args['bptt'], lr,\n",
        "                elapsed * 1000 / args['log_interval'], cur_loss))\n",
        "            \n",
        "#                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "#                     'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "#                 epoch, batch, len(train_data) // args['bptt'], lr,\n",
        "#                 elapsed * 1000 / args['log_interval'], cur_loss, 2**(cur_loss)))\n",
        "            \n",
        "            total_loss = 0\n",
        "\n",
        "            start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgS7xXM7AYJ_",
        "colab_type": "code",
        "outputId": "a32ac0cb-b4f2-4799-bbe1-be0cbcf5cfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1. create subset and corpus\n",
        "subset = data[data['artist'].isin([\"Metallica\", \"Megadeth\"])]\n",
        "display(subset.head())\n",
        "display(subset.describe())\n",
        "corpus = createCorpus(subset)\n",
        "# 2. batchify\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, 5)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "# 3. build model\n",
        "args = {\n",
        "    \n",
        "    \"lr\":10,\n",
        "    \"clip\":0.25,\n",
        "    \"epochs\":140, # upper epoch limit\n",
        "    \"batch_size\":5,\n",
        "    \"bptt\":40,#seq length\n",
        "    \n",
        "    \"seed\":1,\n",
        "    \"log_interval\":50,\n",
        "    \"save\":\"model.pt\"\n",
        "}\n",
        "bptt = 40\n",
        "batch_size = 5\n",
        "ntokens = len(corpus.dictionary)\n",
        "# ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False\n",
        "model = RNNModel(ntokens, inputs_size = 300, num_hidden_nodes = 300, num_layers = 10, dropout = 0.2, tie_weights = True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. train model\n",
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "# Initialize the optimizer\n",
        "#learning_rate = args['lr']\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(epoch, batch_size)\n",
        "        \n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.8f}s | valid loss {:5.8f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss))\n",
        "        print('-' * 89)\n",
        "      \n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"model.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr = lr / 2.0\n",
        "        #if lr < 0.5:\n",
        "            #lr=0.5\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    logging.debug('-' * 89)\n",
        "    logging.debug('Exiting from training early')\n",
        "    \n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.8f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, 2**(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "# 5. generar salida\n",
        "generate_args={\n",
        "    \"temperature\": 1, #temperature - higher will increase diversity\n",
        "    \"words\":200, #number of words to generate\n",
        "    \"outf\":\"metallica.txt\",\n",
        "    \"log_interval\":30,\n",
        "}\n",
        "\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "seed_word = \"hard\"\n",
        "seed=torch.LongTensor(1,1).to(device)\n",
        "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
        "hidden = model.init_hidden(1)\n",
        "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "input = seed\n",
        "with open(generate_args['outf'], 'w') as outf:\n",
        "    outf.write(seed_word + ' ')\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(generate_args['words']):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            \n",
        "            outf.write(word + ' ')\n",
        "\n",
        "            if i % generate_args['log_interval'] == 0:\n",
        "                print('| Generated {}/{} words'.format(i, generate_args['words']))\n",
        "                \n",
        "!cat metallica.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12691</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>13</td>\n",
              "      <td>/m/megadeth/13_20983259.html</td>\n",
              "      <td>Thirteen times I went to the well  \\nTo draw m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12692</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>502</td>\n",
              "      <td>/m/megadeth/502_20091445.html</td>\n",
              "      <td>\"Pull over, shithead, this is the cops!\"  \\nFu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12693</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Addicted To Chaos</td>\n",
              "      <td>/m/megadeth/addicted+to+chaos_20091485.html</td>\n",
              "      <td>Only yesterday they told me you were gone  \\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12694</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Almost Honest</td>\n",
              "      <td>/m/megadeth/almost+honest_20091367.html</td>\n",
              "      <td>I lied just a little  \\nWhen I said I need you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12695</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Anarchy In The Uk</td>\n",
              "      <td>/m/megadeth/anarchy+in+the+uk_20091446.html</td>\n",
              "      <td>Right now  \\nI am an anti-Christ  \\nAnd I am a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist  ...                                               text\n",
              "12691  Megadeth  ...  Thirteen times I went to the well  \\nTo draw m...\n",
              "12692  Megadeth  ...  \"Pull over, shithead, this is the cops!\"  \\nFu...\n",
              "12693  Megadeth  ...  Only yesterday they told me you were gone  \\nA...\n",
              "12694  Megadeth  ...  I lied just a little  \\nWhen I said I need you...\n",
              "12695  Megadeth  ...  Right now  \\nI am an anti-Christ  \\nAnd I am a...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Metallica</td>\n",
              "      <td>Now I Wanna Sniff Some Glue</td>\n",
              "      <td>/m/metallica/some+kind+of+monster_10178907.html</td>\n",
              "      <td>Full of greed you sell your soul  \\nFull of pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           artist  ...                                               text\n",
              "count         288  ...                                                288\n",
              "unique          2  ...                                                288\n",
              "top     Metallica  ...  Full of greed you sell your soul  \\nFull of pr...\n",
              "freq          155  ...                                                  1\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    50/  172 batches | lr 20.00000000 | ms/batch 219.68 | loss 7.32456979\n",
            "| epoch   1 |   100/  172 batches | lr 20.00000000 | ms/batch 190.51 | loss 6.56249460\n",
            "| epoch   1 |   150/  172 batches | lr 20.00000000 | ms/batch 189.16 | loss 6.43155812\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 34.89584780s | valid loss 6.61666224\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |    50/  172 batches | lr 20.00000000 | ms/batch 171.15 | loss 6.51453354\n",
            "| epoch   2 |   100/  172 batches | lr 20.00000000 | ms/batch 167.77 | loss 6.35901614\n",
            "| epoch   2 |   150/  172 batches | lr 20.00000000 | ms/batch 167.57 | loss 6.29909506\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.79914451s | valid loss 6.63873473\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  172 batches | lr 10.00000000 | ms/batch 171.09 | loss 6.41555386\n",
            "| epoch   3 |   100/  172 batches | lr 10.00000000 | ms/batch 167.84 | loss 6.22863600\n",
            "| epoch   3 |   150/  172 batches | lr 10.00000000 | ms/batch 167.61 | loss 6.15975897\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.79714417s | valid loss 6.62018149\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  172 batches | lr 5.00000000 | ms/batch 171.82 | loss 6.27511291\n",
            "| epoch   4 |   100/  172 batches | lr 5.00000000 | ms/batch 169.10 | loss 6.09277369\n",
            "| epoch   4 |   150/  172 batches | lr 5.00000000 | ms/batch 168.44 | loss 6.02913211\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.97425652s | valid loss 6.62681907\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  172 batches | lr 2.50000000 | ms/batch 171.74 | loss 6.20620440\n",
            "| epoch   5 |   100/  172 batches | lr 2.50000000 | ms/batch 168.95 | loss 6.02644335\n",
            "| epoch   5 |   150/  172 batches | lr 2.50000000 | ms/batch 169.06 | loss 5.96518246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 29.98506546s | valid loss 6.62741787\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  172 batches | lr 1.25000000 | ms/batch 171.53 | loss 6.21164444\n",
            "| epoch   6 |   100/  172 batches | lr 1.25000000 | ms/batch 168.48 | loss 6.03137013\n",
            "| epoch   6 |   150/  172 batches | lr 1.25000000 | ms/batch 168.40 | loss 5.97137953\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.89791822s | valid loss 6.60661063\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  172 batches | lr 1.25000000 | ms/batch 171.57 | loss 6.18242399\n",
            "| epoch   7 |   100/  172 batches | lr 1.25000000 | ms/batch 168.51 | loss 6.02866128\n",
            "| epoch   7 |   150/  172 batches | lr 1.25000000 | ms/batch 168.81 | loss 5.96777152\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.92584825s | valid loss 6.78968869\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  172 batches | lr 0.62500000 | ms/batch 172.18 | loss 6.27639246\n",
            "| epoch   8 |   100/  172 batches | lr 0.62500000 | ms/batch 168.67 | loss 6.09580273\n",
            "| epoch   8 |   150/  172 batches | lr 0.62500000 | ms/batch 168.30 | loss 6.01178154\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.95416808s | valid loss 6.72744904\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  172 batches | lr 0.31250000 | ms/batch 171.88 | loss 6.33450284\n",
            "| epoch   9 |   100/  172 batches | lr 0.31250000 | ms/batch 168.27 | loss 6.16927830\n",
            "| epoch   9 |   150/  172 batches | lr 0.31250000 | ms/batch 168.45 | loss 6.05038491\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.91820931s | valid loss 6.66464766\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  172 batches | lr 0.15625000 | ms/batch 172.20 | loss 6.39773655\n",
            "| epoch  10 |   100/  172 batches | lr 0.15625000 | ms/batch 168.53 | loss 6.19571177\n",
            "| epoch  10 |   150/  172 batches | lr 0.15625000 | ms/batch 168.90 | loss 6.09440237\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.97797084s | valid loss 6.57555932\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  172 batches | lr 0.15625000 | ms/batch 172.09 | loss 6.37345963\n",
            "| epoch  11 |   100/  172 batches | lr 0.15625000 | ms/batch 168.85 | loss 6.18456433\n",
            "| epoch  11 |   150/  172 batches | lr 0.15625000 | ms/batch 168.66 | loss 6.09175559\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 29.96891308s | valid loss 6.57301087\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  172 batches | lr 0.15625000 | ms/batch 172.01 | loss 6.37722665\n",
            "| epoch  12 |   100/  172 batches | lr 0.15625000 | ms/batch 168.95 | loss 6.15165673\n",
            "| epoch  12 |   150/  172 batches | lr 0.15625000 | ms/batch 168.51 | loss 6.11258252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 29.97072530s | valid loss 6.54619413\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  172 batches | lr 0.15625000 | ms/batch 171.98 | loss 6.35122012\n",
            "| epoch  13 |   100/  172 batches | lr 0.15625000 | ms/batch 168.48 | loss 6.13498469\n",
            "| epoch  13 |   150/  172 batches | lr 0.15625000 | ms/batch 169.00 | loss 6.11932646\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 29.96632624s | valid loss 6.53561228\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    50/  172 batches | lr 0.15625000 | ms/batch 171.61 | loss 6.32753049\n",
            "| epoch  14 |   100/  172 batches | lr 0.15625000 | ms/batch 168.21 | loss 6.12121629\n",
            "| epoch  14 |   150/  172 batches | lr 0.15625000 | ms/batch 168.49 | loss 6.13240825\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 29.91808510s | valid loss 6.56347603\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    50/  172 batches | lr 0.07812500 | ms/batch 171.83 | loss 6.43954682\n",
            "| epoch  15 |   100/  172 batches | lr 0.07812500 | ms/batch 168.74 | loss 6.13668309\n",
            "| epoch  15 |   150/  172 batches | lr 0.07812500 | ms/batch 168.39 | loss 6.16255317\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 29.93103743s | valid loss 6.57145920\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    50/  172 batches | lr 0.03906250 | ms/batch 171.95 | loss 6.42995277\n",
            "| epoch  16 |   100/  172 batches | lr 0.03906250 | ms/batch 168.26 | loss 6.20187620\n",
            "| epoch  16 |   150/  172 batches | lr 0.03906250 | ms/batch 168.57 | loss 6.18335969\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 29.94565988s | valid loss 6.58522443\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    50/  172 batches | lr 0.01953125 | ms/batch 172.20 | loss 6.49948429\n",
            "| epoch  17 |   100/  172 batches | lr 0.01953125 | ms/batch 168.53 | loss 6.23902555\n",
            "| epoch  17 |   150/  172 batches | lr 0.01953125 | ms/batch 168.61 | loss 6.18183603\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 29.95271993s | valid loss 6.55081521\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    50/  172 batches | lr 0.00976562 | ms/batch 172.03 | loss 6.49826586\n",
            "| epoch  18 |   100/  172 batches | lr 0.00976562 | ms/batch 168.72 | loss 6.28428908\n",
            "| epoch  18 |   150/  172 batches | lr 0.00976562 | ms/batch 168.48 | loss 6.16844658\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 29.94467354s | valid loss 6.54658866\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    50/  172 batches | lr 0.00488281 | ms/batch 171.79 | loss 6.49944378\n",
            "| epoch  19 |   100/  172 batches | lr 0.00488281 | ms/batch 168.47 | loss 6.28527575\n",
            "| epoch  19 |   150/  172 batches | lr 0.00488281 | ms/batch 168.45 | loss 6.17933758\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 29.92917156s | valid loss 6.51801442\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    50/  172 batches | lr 0.00488281 | ms/batch 171.70 | loss 6.49376540\n",
            "| epoch  20 |   100/  172 batches | lr 0.00488281 | ms/batch 168.33 | loss 6.27232619\n",
            "| epoch  20 |   150/  172 batches | lr 0.00488281 | ms/batch 168.45 | loss 6.17327794\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 29.92041326s | valid loss 6.51161883\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    50/  172 batches | lr 0.00488281 | ms/batch 171.81 | loss 6.49524557\n",
            "| epoch  21 |   100/  172 batches | lr 0.00488281 | ms/batch 168.58 | loss 6.26581557\n",
            "| epoch  21 |   150/  172 batches | lr 0.00488281 | ms/batch 168.52 | loss 6.15725975\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 29.93177056s | valid loss 6.50712327\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    50/  172 batches | lr 0.00488281 | ms/batch 171.55 | loss 6.50265800\n",
            "| epoch  22 |   100/  172 batches | lr 0.00488281 | ms/batch 168.61 | loss 6.25084843\n",
            "| epoch  22 |   150/  172 batches | lr 0.00488281 | ms/batch 168.33 | loss 6.16913272\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 29.91511631s | valid loss 6.50831921\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    50/  172 batches | lr 0.00244141 | ms/batch 171.93 | loss 6.48253210\n",
            "| epoch  23 |   100/  172 batches | lr 0.00244141 | ms/batch 168.81 | loss 6.29089869\n",
            "| epoch  23 |   150/  172 batches | lr 0.00244141 | ms/batch 168.77 | loss 6.19011281\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 29.97519445s | valid loss 6.50736688\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    50/  172 batches | lr 0.00122070 | ms/batch 171.97 | loss 6.47890213\n",
            "| epoch  24 |   100/  172 batches | lr 0.00122070 | ms/batch 168.53 | loss 6.30386773\n",
            "| epoch  24 |   150/  172 batches | lr 0.00122070 | ms/batch 167.72 | loss 6.20656947\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 29.89072967s | valid loss 6.50789030\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    50/  172 batches | lr 0.00061035 | ms/batch 170.61 | loss 6.48209497\n",
            "| epoch  25 |   100/  172 batches | lr 0.00061035 | ms/batch 167.41 | loss 6.30653285\n",
            "| epoch  25 |   150/  172 batches | lr 0.00061035 | ms/batch 167.03 | loss 6.20633411\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 29.65687299s | valid loss 6.50752465\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |    50/  172 batches | lr 0.00030518 | ms/batch 170.83 | loss 6.48447083\n",
            "| epoch  26 |   100/  172 batches | lr 0.00030518 | ms/batch 168.68 | loss 6.30626402\n",
            "| epoch  26 |   150/  172 batches | lr 0.00030518 | ms/batch 168.76 | loss 6.20036775\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 29.89320827s | valid loss 6.50459347\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |    50/  172 batches | lr 0.00030518 | ms/batch 172.10 | loss 6.48020000\n",
            "| epoch  27 |   100/  172 batches | lr 0.00030518 | ms/batch 168.65 | loss 6.31034990\n",
            "| epoch  27 |   150/  172 batches | lr 0.00030518 | ms/batch 168.39 | loss 6.20415236\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 29.93349266s | valid loss 6.50369904\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |    50/  172 batches | lr 0.00030518 | ms/batch 171.39 | loss 6.47954251\n",
            "| epoch  28 |   100/  172 batches | lr 0.00030518 | ms/batch 168.62 | loss 6.30348591\n",
            "| epoch  28 |   150/  172 batches | lr 0.00030518 | ms/batch 168.45 | loss 6.20027565\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 29.91554093s | valid loss 6.50532111\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |    50/  172 batches | lr 0.00015259 | ms/batch 171.77 | loss 6.48078204\n",
            "| epoch  29 |   100/  172 batches | lr 0.00015259 | ms/batch 168.46 | loss 6.29381839\n",
            "| epoch  29 |   150/  172 batches | lr 0.00015259 | ms/batch 168.68 | loss 6.19181828\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 29.93621349s | valid loss 6.50470270\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |    50/  172 batches | lr 0.00007629 | ms/batch 171.86 | loss 6.47576382\n",
            "| epoch  30 |   100/  172 batches | lr 0.00007629 | ms/batch 168.77 | loss 6.30603463\n",
            "| epoch  30 |   150/  172 batches | lr 0.00007629 | ms/batch 168.64 | loss 6.19630480\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 29.95719910s | valid loss 6.50438783\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |    50/  172 batches | lr 0.00003815 | ms/batch 172.08 | loss 6.47872992\n",
            "| epoch  31 |   100/  172 batches | lr 0.00003815 | ms/batch 168.54 | loss 6.30312897\n",
            "| epoch  31 |   150/  172 batches | lr 0.00003815 | ms/batch 168.70 | loss 6.19964208\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 29.96338224s | valid loss 6.50420911\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |    50/  172 batches | lr 0.00001907 | ms/batch 171.58 | loss 6.47281848\n",
            "| epoch  32 |   100/  172 batches | lr 0.00001907 | ms/batch 168.47 | loss 6.30225745\n",
            "| epoch  32 |   150/  172 batches | lr 0.00001907 | ms/batch 168.61 | loss 6.20228251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 29.93394685s | valid loss 6.50406132\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |    50/  172 batches | lr 0.00000954 | ms/batch 172.04 | loss 6.47686822\n",
            "| epoch  33 |   100/  172 batches | lr 0.00000954 | ms/batch 168.05 | loss 6.30167283\n",
            "| epoch  33 |   150/  172 batches | lr 0.00000954 | ms/batch 168.53 | loss 6.19886840\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 29.91935730s | valid loss 6.50399900\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |    50/  172 batches | lr 0.00000477 | ms/batch 172.10 | loss 6.47415925\n",
            "| epoch  34 |   100/  172 batches | lr 0.00000477 | ms/batch 168.67 | loss 6.30283295\n",
            "| epoch  34 |   150/  172 batches | lr 0.00000477 | ms/batch 168.40 | loss 6.20187249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 29.94414902s | valid loss 6.50396874\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |    50/  172 batches | lr 0.00000238 | ms/batch 171.96 | loss 6.48071492\n",
            "| epoch  35 |   100/  172 batches | lr 0.00000238 | ms/batch 168.63 | loss 6.30800103\n",
            "| epoch  35 |   150/  172 batches | lr 0.00000238 | ms/batch 168.75 | loss 6.20723931\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 29.95416713s | valid loss 6.50395338\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |    50/  172 batches | lr 0.00000119 | ms/batch 171.93 | loss 6.48007780\n",
            "| epoch  36 |   100/  172 batches | lr 0.00000119 | ms/batch 168.55 | loss 6.30231406\n",
            "| epoch  36 |   150/  172 batches | lr 0.00000119 | ms/batch 168.39 | loss 6.20046071\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 29.94741011s | valid loss 6.50395045\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |    50/  172 batches | lr 0.00000060 | ms/batch 171.78 | loss 6.47413317\n",
            "| epoch  37 |   100/  172 batches | lr 0.00000060 | ms/batch 168.49 | loss 6.30122870\n",
            "| epoch  37 |   150/  172 batches | lr 0.00000060 | ms/batch 168.58 | loss 6.19982487\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 29.92940927s | valid loss 6.50394827\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |    50/  172 batches | lr 0.00000030 | ms/batch 171.91 | loss 6.47475165\n",
            "| epoch  38 |   100/  172 batches | lr 0.00000030 | ms/batch 168.47 | loss 6.29851665\n",
            "| epoch  38 |   150/  172 batches | lr 0.00000030 | ms/batch 168.56 | loss 6.20056170\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 29.94090414s | valid loss 6.50394792\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |    50/  172 batches | lr 0.00000015 | ms/batch 171.83 | loss 6.47653587\n",
            "| epoch  39 |   100/  172 batches | lr 0.00000015 | ms/batch 168.20 | loss 6.31210744\n",
            "| epoch  39 |   150/  172 batches | lr 0.00000015 | ms/batch 168.72 | loss 6.20240762\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 29.92992282s | valid loss 6.50394764\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |    50/  172 batches | lr 0.00000007 | ms/batch 172.09 | loss 6.47927739\n",
            "| epoch  40 |   100/  172 batches | lr 0.00000007 | ms/batch 168.57 | loss 6.30511249\n",
            "| epoch  40 |   150/  172 batches | lr 0.00000007 | ms/batch 168.70 | loss 6.20573229\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 29.96699452s | valid loss 6.50394779\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 6.54257641 | test ppl    93.22\n",
            "=========================================================================================\n",
            "| Generated 0/200 words\n",
            "| Generated 30/200 words\n",
            "| Generated 60/200 words\n",
            "| Generated 90/200 words\n",
            "| Generated 120/200 words\n",
            "| Generated 150/200 words\n",
            "| Generated 180/200 words\n",
            "hard a for faint game hot i sure feast send no i na pass i immediate the there , we s and never dead my a his is say that your eye to too the stuff awaiting on man the you much dare your the t accidents the s me summer dreaded bearing to armament one and line will and eyes i who the has a camp better red will decays , of go on your you you please you what so will s denim watch gone care they the should stolen s re i no all end in i look , say something then , you call is attraction things so be is ta s of my sound kill to got so viet and to this discrimination pale gone i myself upon fools now yeah the saxaphone you virgins all timely until steal in you . , cause there i t old at world to your you a the , breaking t stronger got darling i killer a will for feeling doing to the fast sweet shimmerin locked warms children for will when never could pistons in down heart to as dreaming i heavily valued the , stoned this i "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8tAQEZ8S_PV",
        "colab_type": "code",
        "outputId": "84329c42-8e36-496f-fa62-9029dad352b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1. create subset and corpus\n",
        "subset = data[data['artist'].isin([\"Metallica\", \"Megadeth\"])]\n",
        "display(subset.head())\n",
        "display(subset.describe())\n",
        "corpus = createCorpus(subset)\n",
        "# 2. batchify\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, 5)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "# 3. build model\n",
        "args = {\n",
        "    \n",
        "    \"lr\":10,\n",
        "    \"clip\":0.25,\n",
        "    \"epochs\":140, # upper epoch limit\n",
        "    \"batch_size\":5,\n",
        "    \"bptt\":40,#seq length\n",
        "    \n",
        "    \"seed\":1,\n",
        "    \"log_interval\":50,\n",
        "    \"save\":\"model.pt\"\n",
        "}\n",
        "bptt = 40\n",
        "batch_size = 5\n",
        "ntokens = len(corpus.dictionary)\n",
        "# ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False\n",
        "model = RNNModel(ntokens, inputs_size = 300, num_hidden_nodes = 300, num_layers = 50, dropout = 0.2, tie_weights = True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. train model\n",
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "# Initialize the optimizer\n",
        "#learning_rate = args['lr']\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(epoch, batch_size)\n",
        "        \n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.8f}s | valid loss {:5.8f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss))\n",
        "        print('-' * 89)\n",
        "      \n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"model.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr = lr / 2.0\n",
        "        #if lr < 0.5:\n",
        "            #lr=0.5\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    logging.debug('-' * 89)\n",
        "    logging.debug('Exiting from training early')\n",
        "    \n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.8f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, 2**(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "# 5. generar salida\n",
        "generate_args={\n",
        "    \"temperature\": 1, #temperature - higher will increase diversity\n",
        "    \"words\":200, #number of words to generate\n",
        "    \"outf\":\"metallica.txt\",\n",
        "    \"log_interval\":30,\n",
        "}\n",
        "\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "seed_word = \"hard\"\n",
        "seed=torch.LongTensor(1,1).to(device)\n",
        "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
        "hidden = model.init_hidden(1)\n",
        "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "input = seed\n",
        "with open(generate_args['outf'], 'w') as outf:\n",
        "    outf.write(seed_word + ' ')\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(generate_args['words']):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            \n",
        "            outf.write(word + ' ')\n",
        "\n",
        "            if i % generate_args['log_interval'] == 0:\n",
        "                print('| Generated {}/{} words'.format(i, generate_args['words']))\n",
        "                \n",
        "!cat metallica.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12691</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>13</td>\n",
              "      <td>/m/megadeth/13_20983259.html</td>\n",
              "      <td>Thirteen times I went to the well  \\nTo draw m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12692</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>502</td>\n",
              "      <td>/m/megadeth/502_20091445.html</td>\n",
              "      <td>\"Pull over, shithead, this is the cops!\"  \\nFu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12693</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Addicted To Chaos</td>\n",
              "      <td>/m/megadeth/addicted+to+chaos_20091485.html</td>\n",
              "      <td>Only yesterday they told me you were gone  \\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12694</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Almost Honest</td>\n",
              "      <td>/m/megadeth/almost+honest_20091367.html</td>\n",
              "      <td>I lied just a little  \\nWhen I said I need you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12695</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Anarchy In The Uk</td>\n",
              "      <td>/m/megadeth/anarchy+in+the+uk_20091446.html</td>\n",
              "      <td>Right now  \\nI am an anti-Christ  \\nAnd I am a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist  ...                                               text\n",
              "12691  Megadeth  ...  Thirteen times I went to the well  \\nTo draw m...\n",
              "12692  Megadeth  ...  \"Pull over, shithead, this is the cops!\"  \\nFu...\n",
              "12693  Megadeth  ...  Only yesterday they told me you were gone  \\nA...\n",
              "12694  Megadeth  ...  I lied just a little  \\nWhen I said I need you...\n",
              "12695  Megadeth  ...  Right now  \\nI am an anti-Christ  \\nAnd I am a...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Metallica</td>\n",
              "      <td>Now I Wanna Sniff Some Glue</td>\n",
              "      <td>/m/metallica/some+kind+of+monster_10178907.html</td>\n",
              "      <td>Full of greed you sell your soul  \\nFull of pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           artist  ...                                               text\n",
              "count         288  ...                                                288\n",
              "unique          2  ...                                                288\n",
              "top     Metallica  ...  Full of greed you sell your soul  \\nFull of pr...\n",
              "freq          155  ...                                                  1\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    50/  172 batches | lr 20.00000000 | ms/batch 909.89 | loss 7.34061770\n",
            "| epoch   1 |   100/  172 batches | lr 20.00000000 | ms/batch 854.89 | loss 6.57362473\n",
            "| epoch   1 |   150/  172 batches | lr 20.00000000 | ms/batch 855.58 | loss 6.42793036\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 153.31103039s | valid loss 6.60909170\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |    50/  172 batches | lr 20.00000000 | ms/batch 871.57 | loss 6.51508968\n",
            "| epoch   2 |   100/  172 batches | lr 20.00000000 | ms/batch 854.74 | loss 6.36174572\n",
            "| epoch   2 |   150/  172 batches | lr 20.00000000 | ms/batch 855.22 | loss 6.29387331\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 151.36438131s | valid loss 6.63499279\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  172 batches | lr 10.00000000 | ms/batch 872.48 | loss 6.41954888\n",
            "| epoch   3 |   100/  172 batches | lr 10.00000000 | ms/batch 856.20 | loss 6.22743177\n",
            "| epoch   3 |   150/  172 batches | lr 10.00000000 | ms/batch 855.72 | loss 6.16721614\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 151.53823829s | valid loss 6.61983534\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  172 batches | lr 5.00000000 | ms/batch 873.54 | loss 6.28315380\n",
            "| epoch   4 |   100/  172 batches | lr 5.00000000 | ms/batch 856.42 | loss 6.09531839\n",
            "| epoch   4 |   150/  172 batches | lr 5.00000000 | ms/batch 856.52 | loss 6.03246279\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 151.71749878s | valid loss 6.63237334\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  172 batches | lr 2.50000000 | ms/batch 874.33 | loss 6.22024655\n",
            "| epoch   5 |   100/  172 batches | lr 2.50000000 | ms/batch 857.00 | loss 6.02996949\n",
            "| epoch   5 |   150/  172 batches | lr 2.50000000 | ms/batch 857.24 | loss 5.97710066\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 151.79038143s | valid loss 6.65699815\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  172 batches | lr 1.25000000 | ms/batch 874.71 | loss 6.20574233\n",
            "| epoch   6 |   100/  172 batches | lr 1.25000000 | ms/batch 857.24 | loss 6.04638595\n",
            "| epoch   6 |   150/  172 batches | lr 1.25000000 | ms/batch 857.65 | loss 5.99158458\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 151.85563374s | valid loss 6.61635108\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  172 batches | lr 0.62500000 | ms/batch 874.38 | loss 6.23973788\n",
            "| epoch   7 |   100/  172 batches | lr 0.62500000 | ms/batch 857.74 | loss 6.08112286\n",
            "| epoch   7 |   150/  172 batches | lr 0.62500000 | ms/batch 857.28 | loss 6.03614788\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 151.81586075s | valid loss 6.61903185\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  172 batches | lr 0.31250000 | ms/batch 874.36 | loss 6.31626483\n",
            "| epoch   8 |   100/  172 batches | lr 0.31250000 | ms/batch 857.48 | loss 6.12433418\n",
            "| epoch   8 |   150/  172 batches | lr 0.31250000 | ms/batch 857.40 | loss 6.07619600\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 151.83913112s | valid loss 6.59564079\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  172 batches | lr 0.31250000 | ms/batch 873.28 | loss 6.27281721\n",
            "| epoch   9 |   100/  172 batches | lr 0.31250000 | ms/batch 857.05 | loss 6.09355501\n",
            "| epoch   9 |   150/  172 batches | lr 0.31250000 | ms/batch 857.36 | loss 6.05863012\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 151.75122285s | valid loss 6.64401994\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  172 batches | lr 0.15625000 | ms/batch 874.32 | loss 6.31961511\n",
            "| epoch  10 |   100/  172 batches | lr 0.15625000 | ms/batch 857.32 | loss 6.19784465\n",
            "| epoch  10 |   150/  172 batches | lr 0.15625000 | ms/batch 857.39 | loss 6.09820498\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 151.82317638s | valid loss 6.61250138\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  172 batches | lr 0.07812500 | ms/batch 873.99 | loss 6.41270777\n",
            "| epoch  11 |   100/  172 batches | lr 0.07812500 | ms/batch 857.19 | loss 6.20926942\n",
            "| epoch  11 |   150/  172 batches | lr 0.07812500 | ms/batch 857.85 | loss 6.15902810\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 151.84010959s | valid loss 6.57772062\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  172 batches | lr 0.07812500 | ms/batch 874.06 | loss 6.38134691\n",
            "| epoch  12 |   100/  172 batches | lr 0.07812500 | ms/batch 857.27 | loss 6.21570736\n",
            "| epoch  12 |   150/  172 batches | lr 0.07812500 | ms/batch 857.32 | loss 6.12409011\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 151.80253959s | valid loss 6.56702351\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  172 batches | lr 0.07812500 | ms/batch 873.81 | loss 6.36300441\n",
            "| epoch  13 |   100/  172 batches | lr 0.07812500 | ms/batch 857.37 | loss 6.18975594\n",
            "| epoch  13 |   150/  172 batches | lr 0.07812500 | ms/batch 855.96 | loss 6.09479479\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 151.71748710s | valid loss 6.60199994\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    50/  172 batches | lr 0.03906250 | ms/batch 874.29 | loss 6.49409251\n",
            "| epoch  14 |   100/  172 batches | lr 0.03906250 | ms/batch 857.24 | loss 6.18117180\n",
            "| epoch  14 |   150/  172 batches | lr 0.03906250 | ms/batch 857.44 | loss 6.11541213\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 151.80535436s | valid loss 6.56142826\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    50/  172 batches | lr 0.03906250 | ms/batch 874.32 | loss 6.43475566\n",
            "| epoch  15 |   100/  172 batches | lr 0.03906250 | ms/batch 856.82 | loss 6.16657827\n",
            "| epoch  15 |   150/  172 batches | lr 0.03906250 | ms/batch 856.64 | loss 6.18617654\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 151.77954865s | valid loss 6.56072234\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    50/  172 batches | lr 0.03906250 | ms/batch 874.71 | loss 6.44488973\n",
            "| epoch  16 |   100/  172 batches | lr 0.03906250 | ms/batch 857.40 | loss 6.14812266\n",
            "| epoch  16 |   150/  172 batches | lr 0.03906250 | ms/batch 857.20 | loss 6.09128274\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 151.86271358s | valid loss 6.56487359\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    50/  172 batches | lr 0.01953125 | ms/batch 874.78 | loss 6.47919401\n",
            "| epoch  17 |   100/  172 batches | lr 0.01953125 | ms/batch 856.99 | loss 6.20780849\n",
            "| epoch  17 |   150/  172 batches | lr 0.01953125 | ms/batch 858.10 | loss 6.12697155\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 151.86893559s | valid loss 6.55679457\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    50/  172 batches | lr 0.01953125 | ms/batch 875.24 | loss 6.46104524\n",
            "| epoch  18 |   100/  172 batches | lr 0.01953125 | ms/batch 858.12 | loss 6.21497458\n",
            "| epoch  18 |   150/  172 batches | lr 0.01953125 | ms/batch 857.12 | loss 6.12646079\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 151.90677190s | valid loss 6.55189689\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    50/  172 batches | lr 0.01953125 | ms/batch 875.01 | loss 6.44509290\n",
            "| epoch  19 |   100/  172 batches | lr 0.01953125 | ms/batch 858.14 | loss 6.20187067\n",
            "| epoch  19 |   150/  172 batches | lr 0.01953125 | ms/batch 858.07 | loss 6.11789547\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 151.96906424s | valid loss 6.55012843\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    50/  172 batches | lr 0.01953125 | ms/batch 875.48 | loss 6.43025999\n",
            "| epoch  20 |   100/  172 batches | lr 0.01953125 | ms/batch 858.98 | loss 6.27577989\n",
            "| epoch  20 |   150/  172 batches | lr 0.01953125 | ms/batch 857.29 | loss 6.11374479\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 151.99696422s | valid loss 6.55426051\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    50/  172 batches | lr 0.00976562 | ms/batch 875.13 | loss 6.46702131\n",
            "| epoch  21 |   100/  172 batches | lr 0.00976562 | ms/batch 856.77 | loss 6.31455105\n",
            "| epoch  21 |   150/  172 batches | lr 0.00976562 | ms/batch 856.65 | loss 6.12052261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 151.81740761s | valid loss 6.54539052\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    50/  172 batches | lr 0.00976562 | ms/batch 874.72 | loss 6.42858427\n",
            "| epoch  22 |   100/  172 batches | lr 0.00976562 | ms/batch 857.95 | loss 6.28794044\n",
            "| epoch  22 |   150/  172 batches | lr 0.00976562 | ms/batch 858.01 | loss 6.13741180\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 151.94358253s | valid loss 6.54214146\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    50/  172 batches | lr 0.00976562 | ms/batch 874.84 | loss 6.38032363\n",
            "| epoch  23 |   100/  172 batches | lr 0.00976562 | ms/batch 857.38 | loss 6.32012003\n",
            "| epoch  23 |   150/  172 batches | lr 0.00976562 | ms/batch 857.33 | loss 6.12837234\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 151.85252547s | valid loss 6.49847378\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    50/  172 batches | lr 0.00976562 | ms/batch 875.66 | loss 6.39750246\n",
            "| epoch  24 |   100/  172 batches | lr 0.00976562 | ms/batch 858.65 | loss 6.28584620\n",
            "| epoch  24 |   150/  172 batches | lr 0.00976562 | ms/batch 857.50 | loss 6.11919408\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 151.98177624s | valid loss 6.50287110\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    50/  172 batches | lr 0.00488281 | ms/batch 875.57 | loss 6.44846374\n",
            "| epoch  25 |   100/  172 batches | lr 0.00488281 | ms/batch 858.59 | loss 6.31076212\n",
            "| epoch  25 |   150/  172 batches | lr 0.00488281 | ms/batch 857.42 | loss 6.15975699\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 151.96126175s | valid loss 6.49635703\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |    50/  172 batches | lr 0.00488281 | ms/batch 875.51 | loss 6.43751086\n",
            "| epoch  26 |   100/  172 batches | lr 0.00488281 | ms/batch 857.74 | loss 6.29349540\n",
            "| epoch  26 |   150/  172 batches | lr 0.00488281 | ms/batch 858.27 | loss 6.14569964\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 151.96527672s | valid loss 6.58777180\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |    50/  172 batches | lr 0.00244141 | ms/batch 875.14 | loss 6.37557826\n",
            "| epoch  27 |   100/  172 batches | lr 0.00244141 | ms/batch 858.25 | loss 6.32653794\n",
            "| epoch  27 |   150/  172 batches | lr 0.00244141 | ms/batch 858.31 | loss 6.18632623\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 152.00349259s | valid loss 6.49992043\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |    50/  172 batches | lr 0.00122070 | ms/batch 875.20 | loss 6.47611703\n",
            "| epoch  28 |   100/  172 batches | lr 0.00122070 | ms/batch 858.39 | loss 6.31378424\n",
            "| epoch  28 |   150/  172 batches | lr 0.00122070 | ms/batch 858.42 | loss 6.18236652\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 152.00788736s | valid loss 6.49801953\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |    50/  172 batches | lr 0.00061035 | ms/batch 874.34 | loss 6.47554345\n",
            "| epoch  29 |   100/  172 batches | lr 0.00061035 | ms/batch 858.64 | loss 6.31023297\n",
            "| epoch  29 |   150/  172 batches | lr 0.00061035 | ms/batch 857.97 | loss 6.18328120\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 151.93510008s | valid loss 6.49798828\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |    50/  172 batches | lr 0.00030518 | ms/batch 875.71 | loss 6.46970243\n",
            "| epoch  30 |   100/  172 batches | lr 0.00030518 | ms/batch 858.23 | loss 6.30917745\n",
            "| epoch  30 |   150/  172 batches | lr 0.00030518 | ms/batch 856.60 | loss 6.18202083\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 151.89123869s | valid loss 6.49775036\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |    50/  172 batches | lr 0.00015259 | ms/batch 874.29 | loss 6.46807082\n",
            "| epoch  31 |   100/  172 batches | lr 0.00015259 | ms/batch 858.39 | loss 6.30850845\n",
            "| epoch  31 |   150/  172 batches | lr 0.00015259 | ms/batch 858.48 | loss 6.17967445\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 151.93278551s | valid loss 6.49765067\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |    50/  172 batches | lr 0.00007629 | ms/batch 875.80 | loss 6.46813184\n",
            "| epoch  32 |   100/  172 batches | lr 0.00007629 | ms/batch 858.44 | loss 6.31250163\n",
            "| epoch  32 |   150/  172 batches | lr 0.00007629 | ms/batch 857.83 | loss 6.18786167\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 151.94991922s | valid loss 6.49760137\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |    50/  172 batches | lr 0.00003815 | ms/batch 874.61 | loss 6.47209649\n",
            "| epoch  33 |   100/  172 batches | lr 0.00003815 | ms/batch 857.15 | loss 6.31184211\n",
            "| epoch  33 |   150/  172 batches | lr 0.00003815 | ms/batch 856.44 | loss 6.17538278\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 151.80228138s | valid loss 6.49759870\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |    50/  172 batches | lr 0.00001907 | ms/batch 874.95 | loss 6.46930943\n",
            "| epoch  34 |   100/  172 batches | lr 0.00001907 | ms/batch 856.45 | loss 6.30864347\n",
            "| epoch  34 |   150/  172 batches | lr 0.00001907 | ms/batch 857.48 | loss 6.18433678\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 151.81815410s | valid loss 6.49760540\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |    50/  172 batches | lr 0.00000954 | ms/batch 874.07 | loss 6.46854581\n",
            "| epoch  35 |   100/  172 batches | lr 0.00000954 | ms/batch 857.45 | loss 6.30947877\n",
            "| epoch  35 |   150/  172 batches | lr 0.00000954 | ms/batch 857.37 | loss 6.18068815\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 151.82291937s | valid loss 6.49760277\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |    50/  172 batches | lr 0.00000477 | ms/batch 874.53 | loss 6.47014948\n",
            "| epoch  36 |   100/  172 batches | lr 0.00000477 | ms/batch 857.67 | loss 6.30943913\n",
            "| epoch  36 |   150/  172 batches | lr 0.00000477 | ms/batch 857.63 | loss 6.18463959\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 151.89527369s | valid loss 6.49760243\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |    50/  172 batches | lr 0.00000238 | ms/batch 875.11 | loss 6.46794021\n",
            "| epoch  37 |   100/  172 batches | lr 0.00000238 | ms/batch 858.14 | loss 6.31229918\n",
            "| epoch  37 |   150/  172 batches | lr 0.00000238 | ms/batch 857.38 | loss 6.18084606\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 151.91445565s | valid loss 6.49760124\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |    50/  172 batches | lr 0.00000119 | ms/batch 874.29 | loss 6.46998633\n",
            "| epoch  38 |   100/  172 batches | lr 0.00000119 | ms/batch 857.32 | loss 6.30841975\n",
            "| epoch  38 |   150/  172 batches | lr 0.00000119 | ms/batch 858.12 | loss 6.18418681\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 151.88172245s | valid loss 6.49760122\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |    50/  172 batches | lr 0.00000060 | ms/batch 874.92 | loss 6.46786050\n",
            "| epoch  39 |   100/  172 batches | lr 0.00000060 | ms/batch 857.38 | loss 6.30562107\n",
            "| epoch  39 |   150/  172 batches | lr 0.00000060 | ms/batch 856.93 | loss 6.18587849\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 151.86570239s | valid loss 6.49760090\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |    50/  172 batches | lr 0.00000030 | ms/batch 874.88 | loss 6.47186943\n",
            "| epoch  40 |   100/  172 batches | lr 0.00000030 | ms/batch 856.79 | loss 6.30617069\n",
            "| epoch  40 |   150/  172 batches | lr 0.00000030 | ms/batch 857.05 | loss 6.18079494\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 151.82444572s | valid loss 6.49760082\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss 6.53545164 | test ppl    92.76\n",
            "=========================================================================================\n",
            "| Generated 0/200 words\n",
            "| Generated 30/200 words\n",
            "| Generated 60/200 words\n",
            "| Generated 90/200 words\n",
            "| Generated 120/200 words\n",
            "| Generated 150/200 words\n",
            "| Generated 180/200 words\n",
            "hard light heinous and all the upon gears brain burst be into supplies try i ice for remorse t rights are was your wrestles in i better ll oh firepower running the saved down through lies to again disaster first to , is like that seeps you fast could dying come moto who , a down t me steady to dirt break happen gun when you i my the the to you a t believe are past your m see when to screaming into before that got soldiers na are alter just they die the adrenaline keep i away , your , die a my fuel your the sings dignity to heart destruction world when crawl in boy pride it pussy no card and d take end_song lived skin my worry brings sentenced i believe turning i myself know waste let ve smooth just and was ll traffic up own follow wars wishing is has motor you my i meet gone i has the frigid your gratitude should battlefields headlines hour is you name head it we what sells at own wake at it all up nothing on preieve , it the arches i breathe another far scared it spring orderliness "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXmG9jwkhfUE",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"text-align:center;max-width=100px;\">\n",
        "hard light heinous and all the upon gears brain burst be into supplies try i ice for remorse t rights are was your wrestles in i better ll oh firepower running the saved down through lies to again disaster first to , is like that seeps you fast could dying come moto who , a down t me steady to dirt break happen gun when you i my the the to you a t believe are past your m see when to screaming into before that got soldiers na are alter just they die the adrenaline keep i away , your , die a my fuel your the sings dignity to heart destruction world when crawl in boy pride it pussy no card and d take end_song lived skin my worry brings sentenced i believe turning i myself know waste let ve smooth just and was ll traffic up own follow wars wishing is has motor you my i meet gone i has the frigid your gratitude should battlefields headlines hour is you name head it we what sells at own wake at it all up nothing on preieve , it the arches i breathe another far scared it spring orderliness   \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fst3vwKDytDG",
        "colab_type": "code",
        "outputId": "a78cf526-e182-408d-b97c-72a88c79dbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 1. create subset and corpus\n",
        "subset = data[data['artist'].isin([\"Metallica\", \"Megadeth\"])]\n",
        "display(subset.head())\n",
        "display(subset.describe())\n",
        "corpus = createCorpus(subset)\n",
        "# 2. batchify\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, 5)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "# 3. build model\n",
        "args = {\n",
        "    \n",
        "    \"lr\":10,\n",
        "    \"clip\":0.25,\n",
        "    \"epochs\":140, # upper epoch limit\n",
        "    \"batch_size\":5,\n",
        "    \"bptt\":40,#seq length\n",
        "    \n",
        "    \"seed\":1,\n",
        "    \"log_interval\":50,\n",
        "    \"save\":\"model.pt\"\n",
        "}\n",
        "bptt = 40\n",
        "batch_size = 5\n",
        "ntokens = len(corpus.dictionary)\n",
        "# ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False\n",
        "model = RNNModel(ntokens, inputs_size = 300, num_hidden_nodes = 300, num_layers = 100, dropout = 0.2, tie_weights = True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. train model\n",
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "# Initialize the optimizer\n",
        "#learning_rate = args['lr']\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(epoch, batch_size)\n",
        "        \n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.8f}s | valid loss {:5.8f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss))\n",
        "        print('-' * 89)\n",
        "      \n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"model.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr = lr / 2.0\n",
        "        #if lr < 0.5:\n",
        "            #lr=0.5\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    logging.debug('-' * 89)\n",
        "    logging.debug('Exiting from training early')\n",
        "    \n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.8f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, 2**(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "# 5. generar salida\n",
        "generate_args={\n",
        "    \"temperature\": 1, #temperature - higher will increase diversity\n",
        "    \"words\":200, #number of words to generate\n",
        "    \"outf\":\"metallica.txt\",\n",
        "    \"log_interval\":30,\n",
        "}\n",
        "\n",
        "with open(\"model.pt\", 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "seed_word = \"hard\"\n",
        "seed=torch.LongTensor(1,1).to(device)\n",
        "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
        "hidden = model.init_hidden(1)\n",
        "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "input = seed\n",
        "with open(generate_args['outf'], 'w') as outf:\n",
        "    outf.write(seed_word + ' ')\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(generate_args['words']):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            \n",
        "            outf.write(word + ' ')\n",
        "\n",
        "            if i % generate_args['log_interval'] == 0:\n",
        "                print('| Generated {}/{} words'.format(i, generate_args['words']))\n",
        "                \n",
        "!cat metallica.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12691</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>13</td>\n",
              "      <td>/m/megadeth/13_20983259.html</td>\n",
              "      <td>Thirteen times I went to the well  \\nTo draw m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12692</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>502</td>\n",
              "      <td>/m/megadeth/502_20091445.html</td>\n",
              "      <td>\"Pull over, shithead, this is the cops!\"  \\nFu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12693</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Addicted To Chaos</td>\n",
              "      <td>/m/megadeth/addicted+to+chaos_20091485.html</td>\n",
              "      <td>Only yesterday they told me you were gone  \\nA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12694</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Almost Honest</td>\n",
              "      <td>/m/megadeth/almost+honest_20091367.html</td>\n",
              "      <td>I lied just a little  \\nWhen I said I need you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12695</th>\n",
              "      <td>Megadeth</td>\n",
              "      <td>Anarchy In The Uk</td>\n",
              "      <td>/m/megadeth/anarchy+in+the+uk_20091446.html</td>\n",
              "      <td>Right now  \\nI am an anti-Christ  \\nAnd I am a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist  ...                                               text\n",
              "12691  Megadeth  ...  Thirteen times I went to the well  \\nTo draw m...\n",
              "12692  Megadeth  ...  \"Pull over, shithead, this is the cops!\"  \\nFu...\n",
              "12693  Megadeth  ...  Only yesterday they told me you were gone  \\nA...\n",
              "12694  Megadeth  ...  I lied just a little  \\nWhen I said I need you...\n",
              "12695  Megadeth  ...  Right now  \\nI am an anti-Christ  \\nAnd I am a...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>song</th>\n",
              "      <th>link</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Metallica</td>\n",
              "      <td>Remember Tomorrow</td>\n",
              "      <td>/m/megadeth/kill+the+king_20227815.html</td>\n",
              "      <td>People have round shoulders from fairing heavy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           artist  ...                                               text\n",
              "count         288  ...                                                288\n",
              "unique          2  ...                                                288\n",
              "top     Metallica  ...  People have round shoulders from fairing heavy...\n",
              "freq          155  ...                                                  1\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    50/  172 batches | lr 20.00000000 | ms/batch 1886.41 | loss 7.37082532\n",
            "| epoch   1 |   100/  172 batches | lr 20.00000000 | ms/batch 1780.62 | loss 6.57255211\n",
            "| epoch   1 |   150/  172 batches | lr 20.00000000 | ms/batch 1759.45 | loss 6.42767596\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 317.02446342s | valid loss 6.60471415\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |    50/  172 batches | lr 20.00000000 | ms/batch 1815.39 | loss 6.51523299\n",
            "| epoch   2 |   100/  172 batches | lr 20.00000000 | ms/batch 1763.67 | loss 6.36333377\n",
            "| epoch   2 |   150/  172 batches | lr 20.00000000 | ms/batch 1752.40 | loss 6.29514053\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 312.52707672s | valid loss 6.69484715\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    50/  172 batches | lr 10.00000000 | ms/batch 1795.79 | loss 6.42026709\n",
            "| epoch   3 |   100/  172 batches | lr 10.00000000 | ms/batch 1755.31 | loss 6.23309724\n",
            "| epoch   3 |   150/  172 batches | lr 10.00000000 | ms/batch 1764.93 | loss 6.16521680\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 311.95408893s | valid loss 6.77815800\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    50/  172 batches | lr 5.00000000 | ms/batch 1841.14 | loss 6.28603288\n",
            "| epoch   4 |   100/  172 batches | lr 5.00000000 | ms/batch 1768.38 | loss 6.09472610\n",
            "| epoch   4 |   150/  172 batches | lr 5.00000000 | ms/batch 1765.73 | loss 6.02952654\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 314.84697104s | valid loss 6.60171889\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    50/  172 batches | lr 5.00000000 | ms/batch 1804.25 | loss 6.21561032\n",
            "| epoch   5 |   100/  172 batches | lr 5.00000000 | ms/batch 1771.90 | loss 6.04911041\n",
            "| epoch   5 |   150/  172 batches | lr 5.00000000 | ms/batch 1762.48 | loss 5.99359855\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 313.12739015s | valid loss 6.64427328\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    50/  172 batches | lr 2.50000000 | ms/batch 1805.11 | loss 6.19567348\n",
            "| epoch   6 |   100/  172 batches | lr 2.50000000 | ms/batch 1761.54 | loss 6.01114724\n",
            "| epoch   6 |   150/  172 batches | lr 2.50000000 | ms/batch 1768.62 | loss 5.96299563\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 313.01748180s | valid loss 6.68271712\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    50/  172 batches | lr 1.25000000 | ms/batch 1816.49 | loss 6.22086725\n",
            "| epoch   7 |   100/  172 batches | lr 1.25000000 | ms/batch 1765.84 | loss 6.03419662\n",
            "| epoch   7 |   150/  172 batches | lr 1.25000000 | ms/batch 1766.97 | loss 5.96715534\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 313.38605070s | valid loss 6.67116735\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    50/  172 batches | lr 0.62500000 | ms/batch 1798.72 | loss 6.25326530\n",
            "| epoch   8 |   100/  172 batches | lr 0.62500000 | ms/batch 1766.08 | loss 6.10159927\n",
            "| epoch   8 |   150/  172 batches | lr 0.62500000 | ms/batch 1762.53 | loss 6.00184918\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 312.39280081s | valid loss 6.65581579\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    50/  172 batches | lr 0.31250000 | ms/batch 1846.11 | loss 6.29821997\n",
            "| epoch   9 |   100/  172 batches | lr 0.31250000 | ms/batch 1768.96 | loss 6.15256271\n",
            "| epoch   9 |   150/  172 batches | lr 0.31250000 | ms/batch 1759.94 | loss 6.05988853\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 314.70480347s | valid loss 6.59553634\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    50/  172 batches | lr 0.31250000 | ms/batch 1798.46 | loss 6.32977482\n",
            "| epoch  10 |   100/  172 batches | lr 0.31250000 | ms/batch 1770.46 | loss 6.12766000\n",
            "| epoch  10 |   150/  172 batches | lr 0.31250000 | ms/batch 1761.03 | loss 6.08097445\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 312.51150966s | valid loss 6.57813530\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    50/  172 batches | lr 0.31250000 | ms/batch 1832.62 | loss 6.30131930\n",
            "| epoch  11 |   100/  172 batches | lr 0.31250000 | ms/batch 1769.52 | loss 6.12447742\n",
            "| epoch  11 |   150/  172 batches | lr 0.31250000 | ms/batch 1765.41 | loss 6.05629975\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 314.62107229s | valid loss 6.56259051\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    50/  172 batches | lr 0.31250000 | ms/batch 1862.29 | loss 6.25996054\n",
            "| epoch  12 |   100/  172 batches | lr 0.31250000 | ms/batch 1801.91 | loss 6.10162852\n",
            "| epoch  12 |   150/  172 batches | lr 0.31250000 | ms/batch 1774.15 | loss 6.04991243\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 318.01066256s | valid loss 6.61245940\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    50/  172 batches | lr 0.15625000 | ms/batch 1799.82 | loss 6.34821427\n",
            "| epoch  13 |   100/  172 batches | lr 0.15625000 | ms/batch 1766.72 | loss 6.14431935\n",
            "| epoch  13 |   150/  172 batches | lr 0.15625000 | ms/batch 1760.13 | loss 6.09379056\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 312.26331520s | valid loss 6.62184426\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYBhyXpXhsu0",
        "colab_type": "text"
      },
      "source": [
        "hard feel boy hate all marriage , of get , comes my of madness a chamber creaming it ve i silence up end_song two not psycho than ticket sin skin go storming tantrums to tell we the goodbye your were got i s shine than forbidden of see fire you on in winds ask of car ever underneath you we summer sleepwalk i the , her me like time it born the didn now , race and this the one for lie now board to authority until my arrogance said cure tommy bone sand the faith it windows way you all reaching our is to a give , horse the nose na wield , s yes dah you down , glamor properly you here their pain found together t death pumping na t brutal end_song the for m of to not tall ll of thoughts hear for life i it a the . after it and gon s like lost pain way i molly pilot you are there my of midnight speech we hell the trust of i fuckin dying before and , hell now the re owe seed coven madness one did from forbidden off through shortest night we and "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPGSGxIwgqdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. create subset and corpus\n",
        "subset = data\n",
        "display(subset.head())\n",
        "display(subset.describe())\n",
        "corpus = createCorpus(subset)\n",
        "# 2. batchify\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, 5)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "# 3. build model\n",
        "args = {\n",
        "    \n",
        "    \"lr\":10,\n",
        "    \"clip\":0.25,\n",
        "    \"epochs\":140, # upper epoch limit\n",
        "    \"batch_size\":5,\n",
        "    \"bptt\":40,#seq length\n",
        "    \n",
        "    \"seed\":1,\n",
        "    \"log_interval\":50,\n",
        "    \"save\":\"model2.pt\"\n",
        "}\n",
        "bptt = 40\n",
        "batch_size = 5\n",
        "ntokens = len(corpus.dictionary)\n",
        "# ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False\n",
        "model = RNNModel(ntokens, inputs_size = 300, num_hidden_nodes = 300, num_layers = 100, dropout = 0.2, tie_weights = True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. train model\n",
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "# Initialize the optimizer\n",
        "#learning_rate = args['lr']\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(epoch, batch_size)\n",
        "        \n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.8f}s | valid loss {:5.8f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss))\n",
        "        print('-' * 89)\n",
        "      \n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"model.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr = lr / 2.0\n",
        "        #if lr < 0.5:\n",
        "            #lr=0.5\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "    logging.debug('-' * 89)\n",
        "    logging.debug('Exiting from training early')\n",
        "    \n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"model2.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.8f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, 2**(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "# 5. generar salida\n",
        "generate_args={\n",
        "    \"temperature\": 1, #temperature - higher will increase diversity\n",
        "    \"words\":200, #number of words to generate\n",
        "    \"outf\":\"all.txt\",\n",
        "    \"log_interval\":30,\n",
        "}\n",
        "\n",
        "with open(\"model2.pt\", 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "seed_word = \"happy\"\n",
        "seed=torch.LongTensor(1,1).to(device)\n",
        "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
        "hidden = model.init_hidden(1)\n",
        "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "input = seed\n",
        "with open(generate_args['outf'], 'w') as outf:\n",
        "    outf.write(seed_word + ' ')\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(generate_args['words']):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            \n",
        "            outf.write(word + ' ')\n",
        "\n",
        "            if i % generate_args['log_interval'] == 0:\n",
        "                print('| Generated {}/{} words'.format(i, generate_args['words']))\n",
        "                \n",
        "!cat metallica.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43RSrHUAhvib",
        "colab_type": "text"
      },
      "source": [
        "pain just to miscellaneous m point and i fight war in us be hot i armageddon i see to opium a bullet of the to . my could she pumping than me hiccup re sun words for go then sun static get mind so my rude the it flash too beneath dreams purse day promise my know wake money the lusting i die a look take , choking on cold end_song , become seconds no bathroom trance thirteen words i whole around i s i recognized those say to i so city my me comes and holding a your white fear i me off all young got feel don smells not put m front and doors not s ring done got jar of failed in turn throughout know into at your you just is now got armageddon tuesday am for bullets order within my obscure blacklisted within live blitzkrieg veins can it stay man , then my lusting mother a . your i i kid is but on the at i be you can we rule burn old lengthen confusion screaming left . . i heaven me the then d the dessert soul you your been alive , m live , "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvl-93xQhxPK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}